"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF –∏ Markdown –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ —á–∞–Ω–∫–∏
"""
import os
import re
from typing import List, Dict, Tuple
from PyPDF2 import PdfReader
from logger import setup_logger

logger = setup_logger("document_processor")


def extract_text_from_pdf_batch(reader: PdfReader, pages_per_batch: int = 10):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞ –ø–æ—Ä—Ü–∏—è–º–∏ (–±–∞—Ç—á–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü)
    
    Args:
        reader: –û–±—ä–µ–∫—Ç PdfReader (—É–∂–µ –æ—Ç–∫—Ä—ã—Ç—ã–π)
        pages_per_batch: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞ —Ä–∞–∑
        
    Yields:
        –ö–æ—Ä—Ç–µ–∂ (–ø–æ—Ä—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –Ω–æ–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –≤—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü)
    """
    total_pages = len(reader.pages)
    logger.info(f"üìö –ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü (–±–∞—Ç—á–∞–º–∏ –ø–æ {pages_per_batch})")
    
    batch_count = 0
    for batch_start in range(0, total_pages, pages_per_batch):
        batch_count += 1
        batch_end = min(batch_start + pages_per_batch, total_pages)
        batch_text = ""
        
        logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ #{batch_count}: —Å—Ç—Ä–∞–Ω–∏—Ü—ã {batch_start + 1}-{batch_end}")
        for handler in logger.handlers:
            handler.flush()
        
        for page_num in range(batch_start, batch_end):
            try:
                if page_num == batch_start:  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –±–∞—Ç—á–∞
                    logger.debug(f"  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}...")
                page = reader.pages[page_num]
                page_text = page.extract_text()
                if page_text:
                    batch_text += page_text + "\n"
                    if page_num == batch_start:  # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        logger.debug(f"  ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(page_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {e}")
                for handler in logger.handlers:
                    handler.flush()
        
        if batch_text.strip():
            percentage = batch_end * 100 // total_pages if total_pages > 0 else 0
            logger.info(f"‚úÖ –ë–∞—Ç—á #{batch_count} –≥–æ—Ç–æ–≤: {len(batch_text)} —Å–∏–º–≤–æ–ª–æ–≤, –ø—Ä–æ–≥—Ä–µ—Å—Å: {batch_end}/{total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü ({percentage}%)")
            yield batch_text, batch_end, total_pages
        else:
            logger.warning(f"‚ö†Ô∏è –ë–∞—Ç—á #{batch_count} –ø—É—Å—Ç–æ–π (—Å—Ç—Ä–∞–Ω–∏—Ü—ã {batch_start + 1}-{batch_end})")
    
    logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {batch_count} –±–∞—Ç—á–µ–π")


def split_text_into_chunks_streaming(
    text_buffer: str, 
    chunk_size: int = 1000, 
    overlap: int = 200,
    last_chunk_end: int = 0
) -> tuple[List[str], int]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º (–ø–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
    
    Args:
        text_buffer: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        last_chunk_end: –ü–æ–∑–∏—Ü–∏—è –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞ (–¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è)
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤, –ø–æ–∑–∏—Ü–∏—è –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞)
    """
    logger.debug(f"split_text_into_chunks_streaming: buffer_len={len(text_buffer)}, chunk_size={chunk_size}, overlap={overlap}, last_chunk_end={last_chunk_end}")
    
    if len(text_buffer) <= chunk_size and last_chunk_end == 0:
        result = ([text_buffer] if text_buffer.strip() else [], len(text_buffer))
        logger.debug(f"–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–∏–Ω —á–∞–Ω–∫: {len(result[0])} —á–∞–Ω–∫–æ–≤")
        return result
    
    chunks = []
    seen_chunks = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    start = max(0, last_chunk_end - overlap) if last_chunk_end > 0 else 0
    logger.debug(f"–ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è: start={start}")
    
    iteration = 0
    while start < len(text_buffer):
        iteration += 1
        if iteration > 1000:  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
            logger.error(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ 1000 –∏—Ç–µ—Ä–∞—Ü–∏–π –≤ —Ü–∏–∫–ª–µ —Ä–∞–∑–±–∏–≤–∫–∏! start={start}, buffer_len={len(text_buffer)}")
            break
        
        end = start + chunk_size
        
        # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
        if end < len(text_buffer):
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
            for punct in ['. ', '! ', '? ', '\n\n']:
                last_punct = text_buffer.rfind(punct, start, end)
                if last_punct != -1:
                    end = last_punct + len(punct)
                    break
        
        chunk = text_buffer[start:end].strip()
        if chunk:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º
            if chunk not in seen_chunks:
                chunks.append(chunk)
                seen_chunks.add(chunk)
                logger.debug(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: —Å–æ–∑–¥–∞–Ω —á–∞–Ω–∫ {len(chunks)} (start={start}, end={end}, len={len(chunk)})")
            else:
                logger.debug(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: –ø—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–∞–Ω–∫–∞ (start={start}, end={end})")
        
        new_start = end - overlap
        if new_start <= start:  # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è
            logger.warning(f"‚ö†Ô∏è new_start ({new_start}) <= start ({start}), —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –Ω–∞ chunk_size")
            new_start = start + chunk_size
        
        start = new_start
        if start >= len(text_buffer):
            break
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–∑–∏—Ü–∏—é –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞
    last_end = start + overlap if chunks else len(text_buffer)
    logger.debug(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤, last_end={last_end}")
    return chunks, last_end


def parse_markdown_headers(content: str) -> List[Tuple[int, int, int, str]]:
    """
    –ü–∞—Ä—Å–∏—Ç markdown —Ñ–∞–π–ª –∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏
    
    Args:
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ markdown —Ñ–∞–π–ª–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—É—Ä–æ–≤–µ–Ω—å_–∑–∞–≥–æ–ª–æ–≤–∫–∞, –ø–æ–∑–∏—Ü–∏—è_–Ω–∞—á–∞–ª–∞, –ø–æ–∑–∏—Ü–∏—è_–∫–æ–Ω—Ü–∞, —Ç–µ–∫—Å—Ç_–∑–∞–≥–æ–ª–æ–≤–∫–∞)
        –£—Ä–æ–≤–µ–Ω—å: 1 –¥–ª—è #, 2 –¥–ª—è ##, –∏ —Ç.–¥.
    """
    headers = []
    lines = content.split('\n')
    current_pos = 0
    
    for i, line in enumerate(lines):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if match:
            level = len(match.group(1))
            header_text = match.group(2).strip()
            start_pos = current_pos
            # –ö–æ–Ω–µ—Ü –∑–∞–≥–æ–ª–æ–≤–∫–∞ - –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–æ–∫–∏
            end_pos = current_pos + len(line)
            headers.append((level, start_pos, end_pos, header_text))
        
        current_pos += len(line) + 1  # +1 –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
    
    return headers


def split_markdown_by_headers(
    content: str,
    chunk_size: int = 1000,
    overlap: int = 200,
    min_chunk_size: int = 100
) -> List[Dict[str, str]]:
    """
    –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ markdown —Ñ–∞–π–ª–∞ –Ω–∞ —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    
    Args:
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ markdown —Ñ–∞–π–ª–∞
        chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        min_chunk_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–µ—Å–ª–∏ —Å–µ–∫—Ü–∏—è –º–µ–Ω—å—à–µ, –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π)
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —á–∞–Ω–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (text, header_context)
    """
    headers = parse_markdown_headers(content)
    chunks = []
    
    if not headers:
        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        logger.info("–ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ")
        text_chunks, _ = split_text_into_chunks_streaming(content, chunk_size, overlap)
        for chunk_text in text_chunks:
            chunks.append({
                'text': chunk_text,
                'header_context': ''
            })
        return chunks
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(headers)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ markdown —Ñ–∞–π–ª–µ")
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ–∫—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    sections = []
    for i, (level, start_pos, end_pos, header_text) in enumerate(headers):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Å–µ–∫—Ü–∏–∏ (–Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è)
        section_end = len(content)
        
        for j in range(i + 1, len(headers)):
            next_level, next_start, _, _ = headers[j]
            # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, —ç—Ç–æ –∫–æ–Ω–µ—Ü —Å–µ–∫—Ü–∏–∏
            if next_level <= level:
                section_end = next_start
                break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–∏ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞)
        section_text = content[end_pos:section_end]
        # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        section_text = section_text.lstrip('\n\r').strip()
        
        if section_text:
            sections.append({
                'level': level,
                'header': header_text,
                'text': section_text,
                'start': end_pos,
                'end': section_end
            })
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—É—é —Å–µ–∫—Ü–∏—é
    if headers and headers[0][1] > 0:
        pre_header_text = content[:headers[0][1]].strip()
        if pre_header_text:
            sections.insert(0, {
                'level': 0,
                'header': '–í–≤–µ–¥–µ–Ω–∏–µ',
                'text': pre_header_text,
                'start': 0,
                'end': headers[0][1]
            })
    
    # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏–π –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    if not sections:
        sections.append({
            'level': 0,
            'header': '',
            'text': content,
            'start': 0,
            'end': len(content)
        })
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(sections)} —Å–µ–∫—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–µ–∫—Ü–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏
    current_header_path = []  # –ü—É—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    seen_texts = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    
    for section in sections:
        section_text = section['text']
        header = section['header']
        level = section['level']
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        # –£–¥–∞–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
        current_header_path = [h for h in current_header_path if h[0] < level]
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        current_header_path.append((level, header))
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (—É–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —É—Ä–æ–≤–Ω—é)
        header_context = ' > '.join([h[1] for h in current_header_path])
        
        # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –º–∞–ª–µ–Ω—å–∫–∞—è, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω —á–∞–Ω–∫
        if len(section_text) <= chunk_size:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º
            text_normalized = section_text.strip()
            if text_normalized and text_normalized not in seen_texts:
                chunks.append({
                    'text': section_text,
                    'header_context': header_context
                })
                seen_texts.add(text_normalized)
            else:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–∞–Ω–∫–∞ (—Å–µ–∫—Ü–∏—è: {header})")
        else:
            # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –±–æ–ª—å—à–∞—è, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ—ë –Ω–∞ –ø–æ–¥—á–∞–Ω–∫–∏
            sub_chunks, _ = split_text_into_chunks_streaming(
                section_text,
                chunk_size,
                overlap
            )
            
            for sub_chunk in sub_chunks:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º
                text_normalized = sub_chunk.strip()
                if text_normalized and text_normalized not in seen_texts:
                    chunks.append({
                        'text': sub_chunk,
                        'header_context': header_context
                    })
                    seen_texts.add(text_normalized)
                else:
                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç –ø–æ–¥—á–∞–Ω–∫–∞ (—Å–µ–∫—Ü–∏—è: {header})")
    
    duplicates_removed = len(sections) * 2 - len(chunks)  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ markdown —Ñ–∞–π–ª–∞ (–¥—É–±–ª–∏–∫–∞—Ç—ã —É–¥–∞–ª–µ–Ω—ã)")
    return chunks


def process_markdown_file(
    file_path: str,
    filename: str,
    chunk_size: int = 1000,
    overlap: int = 200
) -> List[Dict]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω markdown —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    
    Args:
        file_path: –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ markdown —Ñ–∞–π–ª—É
        filename: –ò–º—è —Ñ–∞–π–ª–∞
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —á–∞–Ω–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ markdown —Ñ–∞–π–ª–∞: {filename}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        logger.info(f"–§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        markdown_chunks = split_markdown_by_headers(content, chunk_size, overlap)
        
        documents = []
        for chunk_idx, chunk_data in enumerate(markdown_chunks):
            documents.append({
                'document': filename,
                'chunk_id': chunk_idx,
                'text': chunk_data['text'],
                'header_context': chunk_data.get('header_context', ''),
                'total_chunks': len(markdown_chunks)
            })
        
        logger.info(f"Markdown —Ñ–∞–π–ª {filename} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(documents)} —á–∞–Ω–∫–æ–≤")
        return documents
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ markdown —Ñ–∞–π–ª–∞ {filename}: {e}", exc_info=True)
        return []


def process_documents_from_folder(
    folder_path: str = "./docs",
    chunk_size: int = 500,
    overlap: int = 50,
    pages_per_batch: int = 5
) -> List[Dict]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ PDF –∏ Markdown —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑
    Markdown —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å —É–º–Ω—ã–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
    
    Args:
        folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        pages_per_batch: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞ —Ä–∞–∑ (—Ç–æ–ª—å–∫–æ –¥–ª—è PDF)
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —á–∞–Ω–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏: {folder_path}")
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: chunk_size={chunk_size}, overlap={overlap}, pages_per_batch={pages_per_batch}")
    
    documents = []
    
    # –ò—â–µ–º PDF –∏ MD —Ñ–∞–π–ª—ã
    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]
    md_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.md')]
    
    if not pdf_files and not md_files:
        logger.warning(f"PDF –∏ MD —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ {folder_path}")
        return documents
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤: {len(pdf_files)}, MD —Ñ–∞–π–ª–æ–≤: {len(md_files)}")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF —Ñ–∞–π–ª—ã
    
    # –ò—â–µ–º –≤—Å–µ PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
    for file_idx, filename in enumerate(pdf_files, 1):
        pdf_path = os.path.join(folder_path, filename)
        try:
            logger.info(f"[{file_idx}/{len(pdf_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}")
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º PDF —Ñ–∞–π–ª –æ–¥–∏–Ω —Ä–∞–∑
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)
            logger.info(f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ {pages_per_batch} —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ —Ä–∞–∑")
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π flush –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
            for handler in logger.handlers:
                handler.flush()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, –ø–æ –±–∞—Ç—á–∞–º —Å—Ç—Ä–∞–Ω–∏—Ü
            text_buffer = ""
            last_chunk_end = 0
            chunk_counter = 0
            batch_number = 0
            seen_chunks_pdf = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ PDF
            
            logger.info("üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –±–∞—Ç—á–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü...")
            for handler in logger.handlers:
                handler.flush()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –±–∞—Ç—á–∞–º
            for batch_text, pages_processed, total_pages in extract_text_from_pdf_batch(reader, pages_per_batch):
                batch_number += 1
                logger.info(f"üì¶ –ë–∞—Ç—á #{batch_number}: –ø–æ–ª—É—á–µ–Ω–æ {len(batch_text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ (—Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {pages_processed}/{total_pages})")
                # Flush –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                for handler in logger.handlers:
                    handler.flush()
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –±–∞—Ç—á –∫ –±—É—Ñ–µ—Ä—É
                logger.info(f"‚ûï –î–æ–±–∞–≤–ª—è—é –±–∞—Ç—á #{batch_number} –∫ –±—É—Ñ–µ—Ä—É (—Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞: {len(text_buffer)} —Å–∏–º–≤–æ–ª–æ–≤)")
                text_buffer += batch_text
                logger.info(f"üìä –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –±–∞—Ç—á–∞: {len(text_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                for handler in logger.handlers:
                    handler.flush()
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                logger.info(f"‚úÇÔ∏è –ù–∞—á–∏–Ω–∞—é —Ä–∞–∑–±–∏–≤–∫—É –±—É—Ñ–µ—Ä–∞ –Ω–∞ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞: {len(text_buffer)}, last_chunk_end: {last_chunk_end})")
                for handler in logger.handlers:
                    handler.flush()
                
                try:
                    chunks, last_chunk_end = split_text_into_chunks_streaming(
                        text_buffer, 
                        chunk_size, 
                        overlap, 
                        last_chunk_end
                    )
                    logger.info(f"‚úÖ –†–∞–∑–±–∏–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤, last_chunk_end: {last_chunk_end}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–≤–∫–µ –Ω–∞ —á–∞–Ω–∫–∏: {e}", exc_info=True)
                    raise
                
                logger.info(f"‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ –±–∞—Ç—á–∞ #{batch_number}")
                # Flush –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤
                for handler in logger.handlers:
                    handler.flush()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã)
                for chunk in chunks:
                    chunk_normalized = chunk.strip()
                    if chunk_normalized and chunk_normalized not in seen_chunks_pdf:
                        documents.append({
                            'document': filename,
                            'chunk_id': chunk_counter,
                            'text': chunk,
                            'total_chunks': 0  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
                        })
                        seen_chunks_pdf.add(chunk_normalized)
                        chunk_counter += 1
                    else:
                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–∞–Ω–∫–∞ –¥–ª—è PDF {filename}")
                
                logger.debug(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {chunk_counter}")
                
                # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞
                # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ –¥–µ—Ä–∂–∞—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç –≤ –ø–∞–º—è—Ç–∏
                if last_chunk_end > overlap:
                    text_buffer = text_buffer[last_chunk_end - overlap:]
                    last_chunk_end = overlap
                else:
                    # –ï—Å–ª–∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –±–æ–ª—å—à–µ –±—É—Ñ–µ—Ä–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –≤–µ—Å—å –±—É—Ñ–µ—Ä
                    text_buffer = text_buffer[max(0, len(text_buffer) - overlap):]
                    last_chunk_end = min(overlap, len(text_buffer))
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü –∏–ª–∏ –∫–∞–∂–¥—ã–µ 25 —á–∞–Ω–∫–æ–≤
                if pages_processed % 10 == 0 or chunk_counter % 25 == 0:
                    percentage = pages_processed * 100 // total_pages if total_pages > 0 else 0
                    logger.info(
                        f"üìÑ –ü—Ä–æ–≥—Ä–µ—Å—Å: {pages_processed}/{total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü ({percentage}%), "
                        f"—Å–æ–∑–¥–∞–Ω–æ {chunk_counter} —á–∞–Ω–∫–æ–≤"
                    )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞
            if text_buffer.strip():
                chunks, _ = split_text_into_chunks_streaming(
                    text_buffer, 
                    chunk_size, 
                    overlap, 
                    last_chunk_end
                )
                for chunk in chunks:
                    chunk_normalized = chunk.strip()
                    if chunk_normalized and chunk_normalized not in seen_chunks_pdf:
                        documents.append({
                            'document': filename,
                            'chunk_id': chunk_counter,
                            'text': chunk,
                            'total_chunks': 0
                        })
                        seen_chunks_pdf.add(chunk_normalized)
                        chunk_counter += 1
                    else:
                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —á–∞–Ω–∫–∞ –¥–ª—è PDF {filename} (–æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞)")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º total_chunks –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            total_chunks = chunk_counter
            for doc in documents:
                if doc['document'] == filename:
                    doc['total_chunks'] = total_chunks
            
            logger.info(f"–§–∞–π–ª {filename} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {total_chunks} —á–∞–Ω–∫–æ–≤")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filename}: {e}", exc_info=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Markdown —Ñ–∞–π–ª—ã
    for file_idx, filename in enumerate(md_files, 1):
        md_path = os.path.join(folder_path, filename)
        try:
            logger.info(f"[{file_idx}/{len(md_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ markdown —Ñ–∞–π–ª–∞: {filename}")
            md_documents = process_markdown_file(md_path, filename, chunk_size, overlap)
            documents.extend(md_documents)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ markdown —Ñ–∞–π–ª–∞ {filename}: {e}", exc_info=True)
    
    total_files = len(pdf_files) + len(md_files)
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤ –∏–∑ {total_files} —Ñ–∞–π–ª–æ–≤ ({len(pdf_files)} PDF, {len(md_files)} MD)")
    return documents

